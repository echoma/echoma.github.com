---
layout: post
title:  "《深度学习》学习路径记录（学习中更新中）"
date:   2018-03-20 13:26:21 +0800
---

* 目录
{:toc}

# 1. 基础知识学习

## 1.1 《零基础入门深度学习》

* 找了下教程看到了这个号称适合没啥数学基础的程序员看的教程： [零基础入门深度学习](https://www.zybuluo.com/hanbingtao/note/433855)
* 看这个教程需要会使用python，因为教程里的例子代码是python的，文本中偶尔也会出现python的术语。
* 下面记录了阅读这个教程过程中脑海中出现过的疑问。

### 1.1.1 第一讲 感知器

![感知器示意图](http://upload-images.jianshu.io/upload_images/2256672-801d65e79bfc3162.png)

* 为什么一上来讲了个`感知器`？
  > 这是最简单的一个机器学习模型，通过它可以了解机器学习的工作原理。

* `感知器的定义`这部分提到了`激活函数`的概念。激活函数这个名字很难理解，它激活了什么呢？
  >通过这篇[《一文概览深度学习中的激活函数》](https://www.jiqizhixin.com/articles/2017-11-02-26)的第1小节可以了解个大概。被激活函数否定的信号就相当于被杀死了，被肯定的信号就是被激活。

* `感知器还能做什么`这部分提到了`线性分类`和`线性回归`，这两个词是啥玩意？

  > 这两个词是`统计学`里的概念，大概意思就是文章里说的：
  >   线性分类问题是用一根直线（或者说用一个线性函数）就可以把答案进行分类的问题。
  >   线性回归问题是用一根直线（或者说用一个线性函数）逼近或者模拟数据分布的问题。
  > 据我这个不太懂数学的人读到第2讲的理解看，这一讲里的感知器和激活函数是线性分类问题，还谈不上线性回归的问题。因为这里取值只有输入输出都只有0\1的取值，还不能说是线性的、连续的函数。

* `感知器的训练`这里提到了训练公式（也就是编号第12\13\14\15的公式），这公式怎么来的？里面的这些个字母解释的好像有点不清楚？

  > 公式怎么来的，不重要，科学家研究出来的。
  >
  > `b`是偏置项，是英文单词`bias`的首字母。
  >
  > `t`文中称作是`实际值`，其实就是`期望值`的意思，就是正确的输出应该是什么。这里为何用`label`这个单词，我也不知道，很多教程也叫作`标注值`。
  >
  > `η`读音为`eta`，在统计学里就是效率、回归系数，具体的意义可以在下面的代码里体会下。

* 本讲后部的实战代码一定要仔细看下，对理解本节的内容和了解机器学习API的基本样子很有帮助。
  > 类名`Perceptron`就是英文里的感知器一词。

* 实战代码里可以了解到几点：

  1. 先训练，再使用。训练过程其实就是学习过程。
  2. 训练的最基本的输入是`输入向量input_vec`、`期望值label`、`学习率rate`。之所以称为`向量`，是因为输入的数据是有顺序的，不能随便乱序输入。
  3. 训练过程其实可以反复使用同一组输入向量和期望值。实战代码里对4组输入和期望反复使用了10次（iteration=10）
  4. 训练过程中并没有用到`激活函数f(x)`。这里挺奇怪的，这相当于激活函数跟训练没有关系了。这根前面介绍激活函数的文章所描述的不一致了。这可能是暂时这节的情况，我的理解：我们显然预知`输入向量`和`期望值`是符合线性分类问题的，代码里也写死了就是要优化权重和偏置量，所以激活函数就用不到了。如果后面不是简单的权重和偏置量的线性问题，那激活函数可能就要派上用场了。

### 1.1.2 第二讲 线性单元

![线性单元示意图](http://upload-images.jianshu.io/upload_images/2256672-f57602e423d739ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* `线性单元是啥`一小节提到的`可导的线性函数`是啥？
  > 一般线性函数的线都是连续的，连续的函数一定是可导的。所以这里不需要理解什么叫可导，只需要了解一点：原来的阶跃函数是不连续的，无法对训练结果到底有多逼近期望值进行评估，也就无法获得学习的优化方向。
  > 因此需要连续的函数来进行逼近。所以，如果要解答线性问题，激活函数要换成线性函数，这是线性单元的工作基础。

* `线性单元的目标函数`这一小节的重点？

  > 既然前面说线性函数是可以评估训练结果有多逼近期望值的，那就会有个具体的评估函数来给出训练结果和期望值之间的差距。这个目标函数就是就是这里所说的评估函数。训练的目的就是调整目标函数里的权重值w，使目标函数的取得最小值。
  >
  > 这里出现了一个概念，叫做`训练样本`。训练样本是一个特征和标记的元组，其实就是一个输入值及其期望值的组合。

* `梯度下降优化算法`这一节介绍了一些太高逼格的数学公式，看不太懂怎么办？

  > 目前我也没看懂，只是这些公式大概的关系而已了。似乎不影响后面的理解。
  >
  > 但显然前面第一讲里的代码使用的训练方法就是梯度下降优化算法。只是梯度下降优化算法可以评估逼近的程度，可以用在线性问题的学习上。

* `小结`这里总结了了要点，很重要：

  > `模型`就是从`输入特征x`预测`输出y`的那个函数。这一讲的线性单元的模型是一个线性函数，因此叫线性模型。
  >
  > `目标函数`使用来评估训练效果（逼近预期值的程度）的函数，使目标函数得到极值的参数值，就是模型参数的最优值。
  >
  > 这是任何学习算法的核心。

### 1.1.3 第三讲 神经网络和反向传播算法

![全连接神经网络示意图](http://upload-images.jianshu.io/upload_images/2256672-92111b104ce0d571.jpeg)

* `神经元`和`神经网络`的理解：
  > 神经元其实跟线性单元和感知器很像，只是曲线不是线性了，变成曲线了。
  > 神经网络也很好理解，就是多层神经元且相邻两层的神经元之间两两互联形成了一个网。

* `误差项`这个概念怎么理解？
  > 这一讲里引入了一个新的词语`误差项`，但是网上找不到文章介绍到底误差项是干什么用的。这里只能才想着可能是个类似上一讲里偏置量的东西。

* 如何理解反向推导算法？

  > 首先，这里的算法仍然遵循前一讲的原则：算法是用来评估逼近程度的。
  >
  > 其次，为什么这个算法叫做`反向推导`文章里说的很清楚了。另外，这公式怎么来的并不重要，反正是科学家研究出来的。

* 代码可以跳过，目前我是大概看了下，没有全部弄懂。并不影响后面对第4讲的理解，因为后面是完全不同的神经网络类型。

### 1.1.4 第四讲 卷积神经网络

![卷积神经网络示意图](http://upload-images.jianshu.io/upload_images/2256672-a36210f89c7164a7.png)

这一讲讲解的比较清楚，比较好理解。而且`卷积神经网络(CNN)`是目前最火最重要的神经网络类型了。

* 同样，代码我只是大概看了下，我觉着重点就是记住那几个新概念：

  1. 没用上一讲的`sigmoid`函数，换了个`relu`函数。这个函数速度更快，且减轻了梯度小时问题。
  2. 卷积网络的形式：`INPUT -> [[CONV]*N -> POOL?]*M -> [FC]*K`
  3. 卷积网络特点或者说优势：支持更大参数量（`Pooling`、下采样）、利用像素间位置关系（适合图形）、支持更多层数（非全连接，参数少）

* 如何理解卷积网络的`三维层结构`？

  > 这一讲在`卷积神经网络是啥`这一节里先给了个示意图，但图里感觉这个卷积神经网络把输入当做是二维图形了。`Pooling`会产生`Feature Map`，这个Map是个二维的。而同一层会做多次`Pooling`，得到多个`Feature Map`。因此这一层会有多个Map，这多个二维Map一层层叠加在一起形成三维。后文里讲的`D(深度)`就是Map的层数。

